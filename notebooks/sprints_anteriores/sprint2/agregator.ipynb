{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a603da-21e2-4563-885f-a3226170826b",
   "metadata": {},
   "source": [
    "# Notebook para agregar 100 arquivos parquet do arquivo ZIP '06120089' em um único parquet para pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f18c50-d92e-4f04-a7c8-d13487258973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importação bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97502a30-dad7-40e3-9cf7-9966275b7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install scipy.stats\n",
    "!pip install dask dask[dataframe]\n",
    "!pip install tslearn\n",
    "!pip install tensorflow\n",
    "!pip install pocketbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192d1645-c3ff-4ed0-98a1-9b661cc95ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from pocketbase import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884340e-4bb5-4da6-a41c-04325618b367",
   "metadata": {},
   "source": [
    "## Carregamento dos arquivos parquet, seleção das colunas desejadas e concantenação dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58723c33-be39-4b79-85b1-ae18c6ac3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_count_rows_in_parquet_files(file_path, filtered_cols):\n",
    "    total_rows = 0\n",
    "    list_of_dfs = []  # Lista para armazenar os DataFrames lidos\n",
    "\n",
    "    # Lista de arquivos Parquet no diretório especificado\n",
    "    # parquet_files = [file for file in os.listdir(file_path) if file.endswith('.parquet')]\n",
    "\n",
    "    client = Client('http://0.0.0.0:8090')\n",
    "\n",
    "    # Remember to create an admin user before running this code\n",
    "    # Also remember to create a collection named 'parquets' before running this code\n",
    "    # And fill it with at least one document\n",
    "\n",
    "    admin_data = client.admins.auth_with_password('test@test.com', '1234567890')\n",
    "    result = client.collection('parquets').get_list(1, 20, {})\n",
    "\n",
    "    for item in result.items:\n",
    "        id = str(item).split(' ')[1].split('>')[0]\n",
    "        obj = client.collection('parquets').get_one(id=id)\n",
    "        url = \"http://0.0.0.0:8090/api/files/parquets/\" + id + \"/\" + obj.parquet\n",
    "        df = pd.read_parquet(url, engine='pyarrow')\n",
    "        total_rows += len(df)\n",
    "        list_of_dfs.append(df)\n",
    "        print(f\"Arquivo {obj.parquet} tem {len(df)} linhas.\")\n",
    "    \n",
    "    # # Loop para ler cada arquivo Parquet, contar as linhas e armazenar o DataFrame na lista\n",
    "    # for file_name in parquet_files:\n",
    "    #     full_path = os.path.join(file_path, file_name)\n",
    "    #     df = dd.read_parquet(full_path, columns=filtered_cols)\n",
    "    #     total_rows += len(df)\n",
    "    #     list_of_dfs.append(df)\n",
    "    #     print(f\"Arquivo {file_name} tem {len(df)} linhas.\")\n",
    "    \n",
    "    # Concatenar todos os Dask DataFrames em um único Dask DataFrame\n",
    "    concatenated_df = dd.concat(list_of_dfs, interleave_partitions=True)\n",
    "    \n",
    "    return concatenated_df, total_rows\n",
    "\n",
    "# Uso da função\n",
    "file_path = \"primeiro_arquivo_zip/\"\n",
    "filtered_cols = ['recording_time', 'dateDay-1', 'dateMonth-1', 'dateYear-1', 'phaseOfFlight-1',\n",
    "                  'message0418DAA-1','message0422DAA-1','amscHprsovDrivF-1a', 'amscHprsovDrivF-1b',\n",
    "                  'amscHprsovDrivF-2b', 'amscPrsovDrivF-1a',\n",
    "                  'amscPrsovDrivF-1b', 'amscPrsovDrivF-2b',\n",
    "                  'basBleedLowPressF-1a', 'basBleedLowPressF-2b',\n",
    "                  'basBleedLowTempF-1a', 'basBleedLowTempF-2b',\n",
    "                  'basBleedOverPressF-1a', 'basBleedOverPressF-2b',\n",
    "                  'basBleedOverTempF-1a', 'basBleedOverTempF-2b',\n",
    "                  'bleedFavTmCmd-1a', 'bleedFavTmCmd-1b',\n",
    "                  'bleedFavTmCmd-2a', 'bleedFavTmCmd-2b', 'bleedFavTmFbk-1a',\n",
    "                  'bleedFavTmFbk-1b', 'bleedFavTmFbk-2b', 'bleedHprsovCmdStatus-1a',\n",
    "                  'bleedHprsovCmdStatus-1b', 'bleedHprsovCmdStatus-2a',\n",
    "                  'bleedHprsovCmdStatus-2b', 'bleedHprsovOpPosStatus-1a',\n",
    "                  'bleedHprsovOpPosStatus-1b', 'bleedHprsovOpPosStatus-2a',\n",
    "                  'bleedHprsovOpPosStatus-2b', 'bleedMonPress-1a',\n",
    "                  'bleedMonPress-1b', 'bleedMonPress-2a', 'bleedMonPress-2b',\n",
    "                  'bleedOnStatus-1a', 'bleedOnStatus-1b', 'bleedOnStatus-2b',\n",
    "                  'bleedOverpressCas-2a', 'bleedOverpressCas-2b',\n",
    "                  'bleedPrecoolDiffPress-1a', 'bleedPrecoolDiffPress-1b',\n",
    "                  'bleedPrecoolDiffPress-2a', 'bleedPrecoolDiffPress-2b',\n",
    "                  'bleedPrsovClPosStatus-1a', 'bleedPrsovClPosStatus-2a',\n",
    "                  'bleedPrsovFbk-1a']\n",
    "\n",
    "resulting_df, total_rows = concatenate_and_count_rows_in_parquet_files(' ', filtered_cols)\n",
    "print(f\"O número total de linhas em todos os arquivos é: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5188739-03d4-4c33-be02-df9e7f6a37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a491170-ee87-4ac8-9b9e-df5ca2e9e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05192d6-afc0-46ac-9b65-f271ad5fe246",
   "metadata": {},
   "source": [
    "## Remoção de linhas duplicadas com base todas as colunas exceto a 'record_time' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd380dd-632e-4bbc-a58b-2d905d213b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_except_record_time(dask_df):\n",
    "    \"\"\"\n",
    "    Remove linhas duplicadas com base em todas as colunas, exceto 'record_time'.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dask_df: DataFrame Dask de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame Dask com linhas duplicadas removidas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crie uma lista de colunas a serem consideradas ao identificar duplicatas\n",
    "    columns_to_consider = [col for col in dask_df.columns if col != 'record_time']\n",
    "    \n",
    "    # Use o método drop_duplicates do Dask para remover duplicatas com base nas colunas especificadas\n",
    "    #Mantém o primeiro valor duplicado por default\n",
    "    unique_df = dask_df.drop_duplicates(subset=columns_to_consider)\n",
    "    \n",
    "    return unique_df\n",
    "\n",
    "# Uso da função\n",
    "unique_df = remove_duplicates_except_record_time(resulting_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc46613-b42a-404e-b820-31c59323cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de linhas em 'unique_df' (depois da remoção de duplicatas)\n",
    "after_row_count = len(unique_df)\n",
    "\n",
    "# Imprimir os resultado\n",
    "print(f\"Número de linhas depois da remoção de duplicatas: {after_row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d22008-23df-450c-a73d-e3071a726c55",
   "metadata": {},
   "source": [
    "## Remoção de valores NaN das colunas de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a2218f-954e-4153-b4f6-432096f6ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = unique_df\n",
    "\n",
    "columns_to_fill = ['dateYear-1', 'dateMonth-1', 'dateDay-1']\n",
    "data_df[columns_to_fill] = data_df[columns_to_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1d2e2-ddcd-4257-a1e3-d1c5c42347a2",
   "metadata": {},
   "source": [
    "## Criação da coluna 'data_voo' e exclusão das colunas de ano,mês e dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e34bd5-2f97-4c4b-939f-9ef47c99b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_data_voo(row):\n",
    "    year = int(row['dateYear-1'])\n",
    "    month = int(row['dateMonth-1'])\n",
    "    day = int(row['dateDay-1'])\n",
    "\n",
    "    # Verificar se a data é válida\n",
    "    if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "        return f\"{year}-{str(month).zfill(2)}-{str(day).zfill(2)}\"\n",
    "    else:\n",
    "        return pd.NaT  # Retorna \"Not a Timestamp\" para datas inválidas\n",
    "\n",
    "# Assuming data_df is your DataFrame\n",
    "data_df['data_voo'] = data_df.apply(create_data_voo, axis=1)\n",
    "data_df['data_voo'] = pd.to_datetime(data_df['data_voo'], errors='coerce')\n",
    "\n",
    "print(data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45afba-975c-41f4-826e-95118059b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores inválidos em 'dateMonth-1'\n",
    "invalid_months = data_df[data_df['dateMonth-1'].isin(list(range(13, 100)) + [0])]['dateMonth-1']\n",
    "print(\"Invalid months:\", invalid_months)\n",
    "\n",
    "# Verificar NaNs nas colunas de data\n",
    "nan_counts_before = data_df[['dateYear-1', 'dateMonth-1', 'dateDay-1']].isna().sum()\n",
    "print(\"\\nNaN counts before transformation:\")\n",
    "print(nan_counts_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03b6075-5bdc-4f0d-98fd-50ac9fc8c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already defined data_df and performed the necessary transformations\n",
    "\n",
    "# Create a new column 'data_voo'\n",
    "data_df['data_voo'] = data_df.apply(create_data_voo, axis=1)\n",
    "data_df['data_voo'] = pd.to_datetime(data_df['data_voo'], errors='coerce')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_voo_df = data_df.drop(columns=['dateDay-1', 'dateMonth-1', 'dateYear-1'])\n",
    "\n",
    "# Reorder columns\n",
    "cols = data_voo_df.columns.tolist()\n",
    "cols.insert(1, cols.pop(cols.index('data_voo')))\n",
    "data_voo_df = data_voo_df[cols]\n",
    "\n",
    "# Now you can proceed with using data_voo_df\n",
    "new_data_voo_df = data_voo_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76856967-6b48-4093-933b-e33a020a086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_voo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5c20a-3b55-40b1-939f-a52b6d021cba",
   "metadata": {},
   "source": [
    "# Gravação dos dados do DataFrame feito em Dask no DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f956aa15-2bf8-4b9f-a37b-790219fc8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_voo_df.to_parquet('new_data_voo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64644f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# con = pyodbc.connect('postgresql://postgres:yourpassword@localhost:5432/postgres')\n",
    "database_url = 'postgresql://postgresql:yourpassword@localhost:5432/postgres'\n",
    "engine = create_engine(database_url)\n",
    "data_to_db = new_data_voo_df.head()\n",
    "data_to_db.to_sql(\"Data\", engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
