{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções em comum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_target(df):\n",
    "    df['failure'] = np.where((df['message0418DAA-1'] == 1) | (df['message0422DAA-1'] == 1), 1, 0)\n",
    "    df = df.drop(['message0418DAA-1', 'message0422DAA-1'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_constant_columns(df):\n",
    "    constant_columns = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            constant_columns.append(column)\n",
    "\n",
    "    if constant_columns:\n",
    "        df.drop(columns=constant_columns, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_2_unique_values(df):\n",
    "    for column in df.columns:\n",
    "        if column not in ['message0418DAA-1', 'message0422DAA-1', 'ds', 'failure']:\n",
    "            unique_values = df[column].nunique()\n",
    "            if unique_values == 2:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_values(df):\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize(df):\n",
    "    columns_to_scale = [col for col in df.columns if col != 'recording_time']\n",
    "    scaler = MinMaxScaler()\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca(data, target_column_name, n_components=10):\n",
    "    \"\"\"\n",
    "    Perform PCA (Principal Component Analysis) on the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (DataFrame): The input DataFrame containing features and the target variable.\n",
    "    - target_column_name (str): The name of the target column.\n",
    "    - n_components (int): The number of principal components to retain (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - principal_df (DataFrame): DataFrame containing the principal components.\n",
    "    - explained_variances (list): List of explained variances for each principal component.\n",
    "    - loadings_df (DataFrame): DataFrame containing feature loadings on each principal component.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = data.drop(columns=[target_column_name])\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize PCA with the specified number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the scaled data\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create a DataFrame to store the principal components\n",
    "    principal_df = pd.DataFrame(data=principal_components, columns=[f'PC{i + 1}' for i in range(n_components)])\n",
    "\n",
    "    # Get the explained variances\n",
    "    explained_variances = pca.explained_variance_ratio_\n",
    "\n",
    "    # Create a DataFrame to store the feature loadings\n",
    "    loadings = pca.components_.T  # Transpose to match features with components\n",
    "    loadings_df = pd.DataFrame(data=loadings, columns=[f'PC{i + 1}' for i in range(n_components)], index=X.columns)\n",
    "    top_component_indices = (-explained_variances).argsort()[:n_components]\n",
    "\n",
    "    # Create a list of column names associated with the top components\n",
    "    top_component_column_names = [loadings_df.iloc[:, i].idxmax() for i in top_component_indices]\n",
    "\n",
    "    return top_component_column_names, explained_variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_timestamp(df):\n",
    "    start_time = datetime.datetime(2023, 1, 1, 0, 0)\n",
    "    df['ds'] = [start_time + datetime.timedelta(hours=i) for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hour_column(df):\n",
    "    df['hour'] = df.index - df.index[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.read_csv('aggregated_time_series.csv')\n",
    "\n",
    "simplified_df = simplify_target(time_series)\n",
    "dropped_constant_columns = delete_constant_columns(simplified_df)\n",
    "dropped_columns_with_2_unique_values = drop_columns_with_2_unique_values(dropped_constant_columns)\n",
    "filled = fill_nan_values(dropped_columns_with_2_unique_values)\n",
    "normalized = normalize(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_timestamp = create_timestamp(normalized)\n",
    "with_hour = create_hour_column(with_timestamp)\n",
    "feature_importance = with_hour.copy()\n",
    "feature_importance = feature_importance.drop('ds', axis=1)\n",
    "columns, explained_variances = perform_pca(feature_importance, 'failure', n_components=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = with_hour[['ds', 'hour', 'failure'] + columns]\n",
    "time_series.to_csv('time_series_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def oversample(df):\n",
    "    X = df.drop('failure', axis=1)\n",
    "    y = df['failure']\n",
    "\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled['failure'] = y_resampled\n",
    "\n",
    "    class_distribution_original = df['failure'].value_counts()\n",
    "    class_distribution_resampled = df_resampled['failure'].value_counts()\n",
    "\n",
    "    print(\"Class Distribution Before Oversampling:\")\n",
    "    print(class_distribution_original)\n",
    "\n",
    "    print(\"\\nClass Distribution After Oversampling:\")\n",
    "    print(class_distribution_resampled)\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Oversampling:\n",
      "0.0    1562\n",
      "1.0      18\n",
      "Name: failure, dtype: int64\n",
      "\n",
      "Class Distribution After Oversampling:\n",
      "0.0    1562\n",
      "1.0    1562\n",
      "Name: failure, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classifier_data = pd.read_csv('aggregated_classifier.csv')\n",
    "simplified_df = simplify_target(classifier_data)\n",
    "dropped_constant_columns = delete_constant_columns(simplified_df)\n",
    "dropped_columns_with_2_unique_values = drop_columns_with_2_unique_values(dropped_constant_columns)\n",
    "filled = fill_nan_values(dropped_columns_with_2_unique_values)\n",
    "normalized = normalize(filled)\n",
    "oversampled = oversample(normalized)\n",
    "feature_importance = oversampled.copy()\n",
    "columns, explained_variances = perform_pca(feature_importance, 'failure', n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_data = oversampled[['failure'] + columns]\n",
    "classifier_data.to_csv('classifier_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
