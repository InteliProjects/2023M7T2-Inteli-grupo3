{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df):\n",
    "    filtered_cols = ['amscHprsovDrivF-1a', 'amscHprsovDrivF-1b', \n",
    "                 'amscHprsovDrivF-2b', 'amscPrsovDrivF-1a', \n",
    "                 'amscPrsovDrivF-1b', 'amscPrsovDrivF-2b', \n",
    "                 'basBleedLowPressF-1a', 'basBleedLowPressF-2b', \n",
    "                 'basBleedLowTempF-1a', 'basBleedLowTempF-2b', \n",
    "                 'basBleedOverPressF-1a', 'basBleedOverPressF-2b', \n",
    "                 'basBleedOverTempF-1a', 'basBleedOverTempF-2b', \n",
    "                 'bleedFavTmCmd-1a', 'bleedFavTmCmd-1b', \n",
    "                 'bleedFavTmCmd-2a', 'bleedFavTmCmd-2b', 'bleedFavTmFbk-1a', \n",
    "                 'bleedFavTmFbk-1b', 'bleedFavTmFbk-2b', 'bleedHprsovCmdStatus-1a', \n",
    "                 'bleedHprsovCmdStatus-1b', 'bleedHprsovCmdStatus-2a', \n",
    "                 'bleedHprsovCmdStatus-2b', 'bleedHprsovOpPosStatus-1a', \n",
    "                 'bleedHprsovOpPosStatus-1b', 'bleedHprsovOpPosStatus-2a', \n",
    "                 'bleedHprsovOpPosStatus-2b', 'bleedMonPress-1a', \n",
    "                 'bleedMonPress-1b', 'bleedMonPress-2a', 'bleedMonPress-2b', \n",
    "                 'bleedOnStatus-1a', 'bleedOnStatus-1b', 'bleedOnStatus-2b', \n",
    "                 'bleedOverpressCas-2a', 'bleedOverpressCas-2b', \n",
    "                 'bleedPrecoolDiffPress-1a', 'bleedPrecoolDiffPress-1b', \n",
    "                 'bleedPrecoolDiffPress-2a', 'bleedPrecoolDiffPress-2b', \n",
    "                 'bleedPrsovClPosStatus-1a', 'bleedPrsovClPosStatus-2a', \n",
    "                 'bleedPrsovFbk-1a', 'recording_time', 'message0418DAA-1', 'message0422DAA-1']\n",
    "\n",
    "    df_filtered = df[filtered_cols]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_every_n_rows(df, n=72000):\n",
    "    new_rows = []\n",
    "    columns_to_average = [col for col in df.columns if (col != 'recording_time' and col != 'message0418DAA-1' and col != 'message0422DAA-1')]\n",
    "\n",
    "    for i in range(0, len(df), n):\n",
    "        chunk = df.iloc[i:i + n]\n",
    "        if not chunk.empty:\n",
    "            average_row = chunk[columns_to_average].mean(skipna=True)\n",
    "            \n",
    "            for col in ['message0418DAA-1', 'message0422DAA-1']:\n",
    "                if (df[col].notnull() & (df[col] != 0)).any():\n",
    "                    average_row[col] = 1\n",
    "                else:\n",
    "                    average_row[col] = 0\n",
    "            \n",
    "            first_time = chunk['recording_time'].iloc[0]\n",
    "            average_row['recording_time'] = first_time\n",
    "            new_rows.append(average_row)\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df_filtered = filter_columns(df)\n",
    "    df_averaged = average_every_n_rows(df_filtered)\n",
    "    return df_averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '06120033'  \n",
    "final_dask_df = None\n",
    "for filename in os.listdir(folder_path)[30:1000]:\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        result_dask_df = dd.from_pandas(preprocess(file_path), npartitions=1)\n",
    "        if final_dask_df is None:\n",
    "            final_dask_df = result_dask_df\n",
    "        else:\n",
    "            final_dask_df = dd.concat([final_dask_df, result_dask_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/elisa/Documents/machinelearning/aggregated_time_series.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dask_df.to_csv('aggregated_time_series.csv', single_file=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '06120033'\n",
    "final_dask_df = None\n",
    "for filename in os.listdir(folder_path)[1220:1950]:\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        result_dask_df = dd.from_pandas(preprocess(file_path), npartitions=1)\n",
    "        if final_dask_df is None:\n",
    "            final_dask_df = result_dask_df\n",
    "        else:\n",
    "            final_dask_df = dd.concat([final_dask_df, result_dask_df])\n",
    "\n",
    "names = ['TCRF_ARCHIVE_06120033_20220530132603.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20220601151236.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20220603051739.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20220603142239.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20221106150243.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20230103035245.parquet',\n",
    "         'TCRF_ARCHIVE_06120033_20230714050245.parquet']\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename in names:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        result_dask_df = dd.from_pandas(preprocess(file_path), npartitions=1)\n",
    "        final_dask_df = dd.concat([final_dask_df, result_dask_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/elisa/Documents/machinelearning/aggregated_classifier.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dask_df.to_csv('aggregated_classifier.csv', single_file=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
